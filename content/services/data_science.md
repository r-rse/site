---
title: 'Data Science'
date: 2018-11-18T12:33:46+10:00
icon: 'services/line-chart.gif'
draft: false
featured: true
weight: 1
heroHeading: 'Data Science'
heroSubHeading: 'Get help with your Data science R pipelines'
heroBackground: 'services/line-chart.gif'
blendMode: 'hero-image-blend-color-burn'
---


As modern research becomes increasingly computational, leveraging more diverse forms of data and employing programming for data wrangling, modelling and analysis across more domains, it begins to increasingly fits the definition of data science. 

{{< offering >}}

## What I offer

* Project organisation for portability, transparency and orientation.
* Code modularisation and function development.
* Code Optimisation.
* Code testing and validation.
* Exception handling with actionable, human friendly user messages.
* Documentation (function, long form).
* Data cleaning, processing, munging validation and documentation.
* Literate Programming, Report and manuscript writing in Rmarkdown or Quarto.
* Setting you up to write your thesis in R using bookdown.
* Dependency and computational environment management (including containerisation with Docker of Singularity).
* Version control and transparent code sharing (publicly or privately) through online repositories.
* Continuous integration (for testing or automated documentation builds).
* Packaging papers, code and data into Research Compendia.

{{< /offering >}}

## Why bother?

### Modern Research &#8773; Data science

The **open source statistical programming language R**, has a rich history in academia and is increasingly a tool of choice in data science due to its broad statistical capabilities and active community supporting a rich ecosystem of powerful and continuously evolving packages. 

Such blending of freely available programming and statistical capabilities means deploying powerful and sophisticated statistical models becomes far more accessible. With great power however comes great responsibility and it can take more time, effort and skill to develop robust, reproducible and portable analysis pipelines.

**This is where I come in!**

## Why me?

### Experienced in Data Science

However complex your analysis, whether a single script analysis of tabular data to full scale multi step analysis pipelines involving data harvesting, interacting with databases or other software, data cleaning, profiling and validation, data munging, data modelling, prediction, visualisation and analysis reporting, I can provide support for all your Research Data Science needs.

While I expect to work closely with my clients on their research questions, I have broad prior experience in modelling, statistical analysis and prediction including:
- Frequentist statistics including generalised and mixed modelling, timeseries analysis, spatial statistics and more.
- Modern machine learning techniques incl. deep learning.
- Bayesian statistics.

### A research software engineering approach

Ensuring transparency, robustness, efficiency and reproducibility is just as important. Our experience in Research Software Engineering means such considerations are baked in from the start for all our projects. We can also help with refactoring your own project code according to software engineering best practice.


### Research Compendia for publication of research

While Data Science outputs might take a variety of forms, the academic standard for communicating research results remains the academic paper. However...

{{< blockquote source="**John Claerbout** paraphrased in  [Buckheit and Donoho (1995)](https://statweb.stanford.edu/~wavelab/Wavelab_850/wavelab.pdf)">}}

“An article about computational result is advertising, not scholarship.

The actual scholarship is the full software environment, code and data, that produced the result.“


{{< /blockquote >}}

So how do we share and make better use of the code, data and computational enviroment representing the legitimate scholarship outputs underlying a research paper? Enter the concept of the [research compendium](https://research-compendium.science/) as a container for the collection of materials associated with results reported in a research paper. The compendium serves as a means for distributing, managing, and updating the collection.

Should you choose this route for your publications, I can set you up or help you bundle your the code, text and data associated with a publication as a research compendium for greater transparency, easier distribution and versioning and more efficient updating (particularly useful during the review process!)

#### Some potential features of a research compendium:

- Use of literate programming (in Rmarkdown or Quarto) to combine code, text and outputs like tables and graphs in a single reproducible document.
- Packaging up associated code like functions or data processing scripts for ease of access.
- Use of formal version control system and remote sharing platforms like GitHub for better tracking of changes and ease of distribution. 
- Use of formal release system to improve reproducibility and provencance tracking of state of code used to produce results.
- Inclusion of tests for code validation and increased robustness.
- Appropriate dependency and computational environment management for increased reproducibility.
- Metadata generation for associated data to increase the potential for and ensure correct reuse.


 